{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and Imports"
      ],
      "metadata": {
        "id": "iDYCdaaEQiAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xIc_l5bhiQ5",
        "outputId": "a0705b8e-22ce-42d3-d0c3-0ee6965cad3c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2023.11.17)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import random\n",
        "import json\n",
        "import re\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import csv\n",
        "import gdown\n",
        "import openai"
      ],
      "metadata": {
        "id": "1vnrUT3oK5hN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection\n",
        "\n",
        "The data is downloaded and gathered in our Google Drive as public .xls and .xlsx files. The code below merges them into one .csv file. (No need to run this anymore, since we already have the csv file in a google drive link)"
      ],
      "metadata": {
        "id": "o_jIy4qzTuJo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example list of file IDs - replace with actual file IDs from Google Drive\n",
        "file_ids = [\n",
        "    '1k84aAAxrEQak7fgXXT1_ug2idu_B9a-m',\n",
        "    '17cq-EPUq-c7mqqmDzouNsZ2jeIVeH04b',\n",
        "    '1B5wEALt69dv3M3yFlgPjSvI5rK-eukzQ',\n",
        "    '1PVFv4QPGNED3J01QXg-eyhAUNb1wZxoe',\n",
        "    '1d8ozmri6Q5pURnAcCBfPDoT7G0ivNiKw',\n",
        "    '1KL32QDe5YBEiG0WX2PuGowqr3yVzDbUe',\n",
        "    '1RqdrLE1NuUjqCTSEVA4N_t252Wxp7nYZ',\n",
        "    '1P_biFh4nCdCgS-2pEJI3KRlVEHAtWY61',\n",
        "    '1HqkuYpiKP97QpMVnroPDoZfRPPviNBRM',\n",
        "    '1fHEVlftkaAha4B1tTaeTeQ2KdKaIElHa',\n",
        "    '11mxza_ukPVqP-2n7yJ6JqgzPA-48knAr',\n",
        "    '1pv63r_Lr0dSJY8R6hdHFvROBAqeWi8aa',\n",
        "    '1BiNdh8p1Zf6SsrYRfvJN4b4p2OHhYxpk',\n",
        "    '18FgZMmRbYPsMmvdNUWLfKtgggfZrHWvM',\n",
        "    '13_CQeYTdQDahocTXt4wNu6iMYtDX8maS',\n",
        "    '1ohTWOm0dVqB26ppHGmNKyUFr6_lhU_Yi',\n",
        "    '1rf7jY55s7ndpjxYFpqKiwv8AhXngoFoF',\n",
        "    '1aNfVC27OKfDneov3Ushvf8Qe-wu2JGKW',\n",
        "    '1s50rwFLVPdjXqjOSVuRQz1bzvsmbQ2o8',\n",
        "    '1MgruYiUcxfHQQM5nbLHdhgz_NmZtVeBu',\n",
        "    '1hQ-2WkY5H1IWn351b74rmA5pghlUFDVS',\n",
        "    '19dfdFDHbe4DTYdfphG4QigvvXVRZ2bdT',\n",
        "    '1Dtl5CzqenuLw247HbSD4yYXWTfmo7nH-'\n",
        "]\n",
        "\n",
        "all_data = pd.DataFrame()\n",
        "\n",
        "for file_id in tqdm(file_ids):\n",
        "    # Construct the direct download URL\n",
        "    download_url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "    # Download the file\n",
        "    response = requests.get(download_url)\n",
        "\n",
        "    # Check if the download was successful\n",
        "    if response.status_code == 200:\n",
        "        try:\n",
        "            # Try reading as an xlsx file\n",
        "            df = pd.read_excel(BytesIO(response.content))\n",
        "        except ValueError:\n",
        "            try:\n",
        "                # If that fails, try reading as an older xls file\n",
        "                df = pd.read_excel(BytesIO(response.content), engine='xlrd')\n",
        "            except Exception as e:\n",
        "                print(f\"Error reading file with ID {file_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Append the DataFrame to the combined data\n",
        "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
        "    else:\n",
        "        print(f\"Failed to download file with ID {file_id}\")\n",
        "\n",
        "# Export the combined DataFrame to CSV\n",
        "all_data.to_csv('combined_tennis_matches.csv', index=False)\n"
      ],
      "metadata": {
        "id": "gIecJNMG7_2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "Transfer the data in the csv file to a pandas DataFrame (we already have the csv file in a google drive link)"
      ],
      "metadata": {
        "id": "lX3hum9uT4AG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BpgDNKo0o7VJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ff6508-66a2-46b0-83a2-df2bc8558116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1nlzwPoTlosd4qGW9mA1by05UK6xT06n-\n",
            "To: /content/combined_tennis_matches.csv\n",
            "100%|██████████| 482M/482M [00:04<00:00, 100MB/s]\n"
          ]
        }
      ],
      "source": [
        "file_id = '1nlzwPoTlosd4qGW9mA1by05UK6xT06n-'\n",
        "\n",
        "# URL to download the file\n",
        "url = f'https://drive.google.com/uc?id={file_id}'\n",
        "\n",
        "output_file = 'combined_tennis_matches.csv'\n",
        "gdown.download(url, output_file, quiet=False)\n",
        "\n",
        "# Read the downloaded CSV file into a DataFrame\n",
        "df = pd.read_csv(output_file, low_memory=False)\n",
        "\n",
        "# Convert columns to numeric, coercing errors\n",
        "numeric_cols = ['ATP', 'Best of', 'WRank', 'LRank', 'WPts', 'LPts',\n",
        "                'W1', 'L1', 'W2', 'L2', 'W3', 'L3', 'W4', 'L4', 'W5', 'L5',\n",
        "                'Wsets', 'Lsets', 'B365W', 'B365L', 'PSW', 'PSL',\n",
        "                'MaxW', 'MaxL', 'AvgW', 'AvgL']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(df))\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSMkYaizK_AI",
        "outputId": "d8025e36-293e-457d-dedc-63c389902c7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "830760\n",
            "Index(['ATP', 'Location', 'Tournament', 'Date', 'Series', 'Court', 'Surface',\n",
            "       'Round', 'Best of', 'Winner', 'Loser', 'WRank', 'LRank', 'WPts', 'LPts',\n",
            "       'W1', 'L1', 'W2', 'L2', 'W3', 'L3', 'W4', 'L4', 'W5', 'L5', 'Wsets',\n",
            "       'Lsets', 'Comment', 'B365W', 'B365L', 'PSW', 'PSL', 'MaxW', 'MaxL',\n",
            "       'AvgW', 'AvgL', 'EXW', 'EXL', 'LBW', 'LBL', 'SJW', 'SJL', 'UBW', 'UBL',\n",
            "       'pl1_flag', 'pl1_year_pro', 'pl1_weight', 'pl1_height', 'pl1_hand',\n",
            "       'pl2_flag', 'pl2_year_pro', 'pl2_weight', 'pl2_height', 'pl2_hand',\n",
            "       'Summary'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build samples to fine-tune the LLM (Chatcompletion or completion for gpt-3.5-turbo and davinci, respectively)"
      ],
      "metadata": {
        "id": "2U-tqEc3UfMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN FOR TURBO\n",
        "\n",
        "def create_chat_format(row):\n",
        "    # Function to format set scores\n",
        "    def format_set_score(w, l):\n",
        "        if pd.isna(w) or pd.isna(l):\n",
        "            return ''\n",
        "        else:\n",
        "            return f\"{int(w)}-{int(l)}, \"\n",
        "\n",
        "    # Creating the score string\n",
        "    score_str = ''.join([format_set_score(row[f'W{i}'], row[f'L{i}']) for i in range(1, 6)])\n",
        "    score_str = score_str.rstrip(', ')\n",
        "\n",
        "    # Randomly choose whether to list winner or loser first\n",
        "    if random.choice([True, False]):\n",
        "        first_player, second_player = row['Winner'], row['Loser']\n",
        "        first_player_rank, second_player_rank = row['WRank'], row['LRank']\n",
        "        first_player_pts, second_player_pts = row['WPts'], row['LPts']\n",
        "        first_player_odds, second_player_odds = row['B365W'], row['B365L']\n",
        "        winner_response = row['Winner']\n",
        "    else:\n",
        "        first_player, second_player = row['Loser'], row['Winner']\n",
        "        first_player_rank, second_player_rank = row['LRank'], row['WRank']\n",
        "        first_player_pts, second_player_pts = row['LPts'], row['WPts']\n",
        "        first_player_odds, second_player_odds = row['B365L'], row['B365W']\n",
        "        winner_response = row['Winner']\n",
        "\n",
        "    # Create a system message setting the context\n",
        "    context = (f\"Discuss the following tennis match: {first_player} (Rank: {first_player_rank}, Points: {first_player_pts}, Odds: {first_player_odds}) \"\n",
        "                f\"vs {second_player} (Rank: {second_player_rank}, Points: {second_player_pts}, Odds: {second_player_odds}) \"\n",
        "                f\"on {row['Date']} during the {row['Tournament']} on a {row['Surface']} surface.\")\n",
        "\n",
        "    # Create user question\n",
        "    user_question =  f\"Who won the match between {first_player} and {second_player} on {row['Date']}?\"\n",
        "\n",
        "    assistant_answer = f\"{winner_response} won the match against {row['Loser']}\"\n",
        "\n",
        "\n",
        "    return [context, user_question, assistant_answer]\n",
        "\n",
        "# Initialize list to store the chat data\n",
        "chat_data_list = []\n",
        "\n",
        "# Iterate over the DataFrame rows\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    chat_format = create_chat_format(row)\n",
        "    # Add a new dictionary for each set of context, question, and answer\n",
        "    chat_data_list.append({\"context\": chat_format[0], \"question\": chat_format[1], \"answer\": chat_format[2]})\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "chat_data = pd.DataFrame(chat_data_list)\n",
        "\n",
        "print(chat_data.iloc[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lBkGFOftREG",
        "outputId": "b99b5009-f3c7-4541-c83e-0d49b7fe80eb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 830760/830760 [02:31<00:00, 5467.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN FOR DAVINCI\n",
        "\n",
        "import random\n",
        "\n",
        "def create_summary_and_qa(row):\n",
        "    # Your existing format_set_score function\n",
        "    def format_set_score(w, l):\n",
        "        if pd.isna(w) or pd.isna(l):  # Check if either score is NaN\n",
        "            return ''  # Return an empty string for NaN scores\n",
        "        else:\n",
        "            return f\"{int(w)}-{int(l)}, \"  # Format as integers\n",
        "\n",
        "    # Creating the score string\n",
        "    score_str = ''.join([format_set_score(row[f'W{i}'], row[f'L{i}']) for i in range(1, 6)])\n",
        "    score_str = score_str.rstrip(', ')\n",
        "\n",
        "    # Create summary\n",
        "    summary = (f\"On {row['Date']}, in the {row['Round']} of {row['Tournament']} which is part of the {row['Series']} series, \"\n",
        "               f\"{row['Winner']} defeated {row['Loser']} with a score of {score_str} on {row['Surface']} surface. \"\n",
        "               f\"The match was {'closely contested' if row['B365W'] == row['B365L'] else 'expected to be in favor of the winner'} \"\n",
        "               f\"with betting odds of {row['B365W']} for {row['Winner']} and {row['B365L']} for {row['Loser']}.\")\n",
        "\n",
        "    def create_question(row):\n",
        "        # Assemble statistics for both players\n",
        "        stats = (f\"Player {row['Winner' if random.choice([True, False]) else 'Loser']} (Rank: {row['WRank' if row['Winner'] else 'LRank']}, Points: {row['WPts' if row['Winner'] else 'LPts']}) \"\n",
        "            f\"is playing Player {row['Loser' if row['Winner'] else 'Winner']} (Rank: {row['LRank' if row['Winner'] else 'WRank']}, Points: {row['LPts' if row['Winner'] else 'WPts']}). \"\n",
        "            f\"The match will take place on a {row['Surface']} surface at the {row['Tournament']}, \"\n",
        "            f\"{row['Location']}. The betting odds are {row['B365W' if row['Winner'] else 'B365L']} for {row['Winner' if row['Winner'] else 'Loser']} and {row['B365L' if row['Winner'] else 'B365W']} for {row['Loser' if row['Winner'] else 'Winner']}.\")\n",
        "\n",
        "\n",
        "        # Create the question\n",
        "        question = f\"On {row['Date']}, {stats} Who do you think will win this match?\"\n",
        "\n",
        "        return question\n",
        "\n",
        "    question = create_question(row)\n",
        "    # The answer is the winner's name\n",
        "    answer = row['Winner']\n",
        "\n",
        "    return summary, question, answer\n",
        "\n",
        "# Initialize lists to store the data\n",
        "summaries = []\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "# Iterate over the DataFrame rows with a progress bar\n",
        "for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "    summary, question, answer = create_summary_and_qa(row)\n",
        "    summaries.append(summary)\n",
        "    questions.append(question)\n",
        "    answers.append(answer)\n",
        "\n",
        "# Assign the new data to the DataFrame\n",
        "df['Summary'] = summaries\n",
        "df['Question'] = questions\n",
        "df['Answer'] = answers\n"
      ],
      "metadata": {
        "id": "lBzybQsc-c32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT API\n",
        "\n",
        "If training is too large, only use a fraction of the data. Split into train/test"
      ],
      "metadata": {
        "id": "9RsMWRY1WHxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the API key\n",
        "openai.api_key = '' # fill in your openai secret key"
      ],
      "metadata": {
        "id": "Mfn8fBh-cDDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a fraction of data\n",
        "df_use_now = chat_data.sample(frac=0.091, random_state=25)\n",
        "df_hold_for_later = chat_data.drop(df_use_now.index)\n",
        "\n",
        "# Split into train/test\n",
        "train_df = df_use_now.sample(frac=0.8, random_state=25)\n",
        "test_df = df_use_now.drop(train_df.index)"
      ],
      "metadata": {
        "id": "piRFQGMnVMvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Save the train and test samples to jsonl format for API\n",
        "def save_to_jsonl_chat_format(df, file_path):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "            conversation = {\"messages\": []}\n",
        "            # Add each message to the conversation\n",
        "            conversation[\"messages\"].append({\"role\": \"system\", \"content\": row['context']})\n",
        "            conversation[\"messages\"].append({\"role\": \"user\", \"content\": row['question']})\n",
        "            conversation[\"messages\"].append({\"role\": \"assistant\", \"content\": row['answer']})\n",
        "            file.write(json.dumps(conversation) + '\\n')\n",
        "\n",
        "save_to_jsonl_chat_format(train_df, 'train_data.jsonl')\n",
        "save_to_jsonl_chat_format(test_df, 'test_data.jsonl')\n"
      ],
      "metadata": {
        "id": "ZvI6JJ2ivDfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload training data for finetuning\n",
        "upload_response = openai.File.create(\n",
        "    file=open(\"train_data.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "\n",
        "# Store the file ID for later use\n",
        "train_file_id = upload_response['id']"
      ],
      "metadata": {
        "id": "YovVFRcQWdNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpt-3.5-turbo\n",
        "\n",
        "fine_tune_response = openai.FineTuningJob.create(\n",
        "    training_file=train_file_id,  # Use the ID of the training file\n",
        "    model=\"gpt-3.5-turbo\"  # Specify the model\n",
        ")\n",
        "\n",
        "# Get the fine-tuning job ID\n",
        "turbo_fine_tune_job_id = fine_tune_response['id']"
      ],
      "metadata": {
        "id": "6EmjdePVslc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Davinci\n",
        "\n",
        "fine_tune_response = openai.FineTune.create(\n",
        "    training_file=train_file_id,  # Use the ID of the training file\n",
        "    model='davinci',  # Choose from 'ada', 'babbage', 'curie', or 'davinci'\n",
        "    n_epochs=4,  # Number of training epochs\n",
        "    # Include other parameters as needed\n",
        ")\n",
        "\n",
        "# Get the fine-tuning job ID\n",
        "davinci_fine_tune_job_id = fine_tune_response['id']"
      ],
      "metadata": {
        "id": "hAAOsjfQWfe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# OPTIONAL - to check the status of the fine-tuning job every 10 seconds\n",
        "\n",
        "import time\n",
        "\n",
        "fine_tune_job_id = turbo_fine_tune_job_id # or davinci\n",
        "\n",
        "def check_fine_tune_status(job_id):\n",
        "    status_response = openai.FineTune.retrieve(id=job_id)\n",
        "    return status_response['status']\n",
        "\n",
        "while True:\n",
        "    status = check_fine_tune_status(fine_tune_job_id)\n",
        "    print(f\"Fine-tuning job status: {status}\")\n",
        "\n",
        "    # Break the loop if the job is completed or failed\n",
        "    if status in [\"succeeded\", \"failed\"]:\n",
        "        break\n",
        "\n",
        "    time.sleep(10)\n",
        "\n",
        "print(\"Fine-tuning job completed or failed.\")"
      ],
      "metadata": {
        "id": "75VMo9wxWxxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "job_details = openai.FineTune.retrieve(id=fine_tune_job_id)\n",
        "\n",
        "print(job_details)"
      ],
      "metadata": {
        "id": "Ovho-XUzJSCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "f1Y3s-PVj5yW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR DAVINCI FORMAT\n",
        "\n",
        "# Load the test set\n",
        "test_df = pd.read_json('test_data.jsonl', lines=True)\n",
        "\n",
        "responses = []\n",
        "\n",
        "for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
        "      # Extract the messages, excluding the assistant's response\n",
        "      messages = [message for message in row['messages'] if message['role'] != 'assistant']\n",
        "      print(messages)\n",
        "\n",
        "      response = openai.ChatCompletion.create(\n",
        "          model=\"ft:gpt-3.5-turbo-0613:personal::8SzDtWEd\",  # Model name\n",
        "          messages=messages,\n",
        "          max_tokens=50  # Length of response\n",
        "      )\n",
        "      responses.append(response.choices[0].message['content'].strip())\n",
        "      print(response.choices[0].message['content'].strip())\n",
        "\n",
        "      time.sleep(1)  # sleep to avoid rate limiting\n",
        "\n",
        "# Add responses to the test DataFrame\n",
        "test_df['ModelResponse'] = responses\n"
      ],
      "metadata": {
        "id": "ojPJ8mE10iI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(test_df)"
      ],
      "metadata": {
        "id": "CxZ45yZu1dPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FOR GPT-3.5-TURBO\n",
        "test_df = pd.read_json('test_data.jsonl', lines=True)  # Assuming the file is in JSON Lines format\n",
        "\n",
        "responses = []\n",
        "\n",
        "for index, row in tqdm(test_df.iterrows(), total=test_df.shape[0]):\n",
        "    # Parse the string representation of the list in 'messages' into an actual list\n",
        "    messages_str = row['messages']\n",
        "    try:\n",
        "        # Convert the 'messages' string to an actual JSON object\n",
        "        messages_list = json.loads(messages_str.replace(\"'\", '\"'))\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Failed to decode JSON for row {index}: {e}\")\n",
        "        continue  # Skip this row and move to the next\n",
        "\n",
        "    # Extracting system and user messages to form the prompt\n",
        "    system_content = next(m for m in messages_list if m['role'] == 'system')['content']\n",
        "    user_content = next(m for m in messages_list if m['role'] == 'user')['content']\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_content},\n",
        "        {\"role\": \"user\", \"content\": user_content}\n",
        "    ]\n",
        "    print(messages)\n",
        "\n",
        "    # Generating a response from the model using chat completions endpoint\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"ft:gpt-3.5-turbo-0613:personal::8TJfgWFZ\",\n",
        "        messages=messages,\n",
        "        max_tokens=50  # Adjust as needed\n",
        "    )\n",
        "    print(response.choices[0].message['content'].strip())\n",
        "    responses.append(response.choices[0].message['content'].strip())\n",
        "\n",
        "# Add responses to the test DataFrame\n",
        "test_df = responses\n"
      ],
      "metadata": {
        "id": "lKlkrgoXqj6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(responses)"
      ],
      "metadata": {
        "id": "PYBuI8GQ3Oln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy calculations\n",
        "\n",
        "Note: Sometimes, we had some trouble calculating the test accuracy due to formatting issues, so accuracy was manually calculated among a smaller random test dataset of 361 matches."
      ],
      "metadata": {
        "id": "nfP5HRWXXnDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV files\n",
        "test_df = pd.read_csv('test_data_chat_format.csv')\n",
        "model_df = pd.read_csv('gpt_model_with_responses.csv')\n",
        "\n",
        "# Ensure both DataFrames have the same number of rows\n",
        "if len(test_df) != len(model_df):\n",
        "    print(\"The DataFrames have different lengths. Make sure both CSV files have the same number of rows.\")\n",
        "else:\n",
        "    correct_count = 0\n",
        "\n",
        "    for i in range(len(test_df)):\n",
        "        # Extract assistant's response from test_df\n",
        "        assistant_response = eval(test_df.loc[i, 'messages'])[-1]['content']\n",
        "\n",
        "        # Extract model's response from model_df\n",
        "        model_response = model_df.loc[i, 'ModelResponse']\n",
        "\n",
        "        # Compare responses\n",
        "        if assistant_response.strip().lower() == model_response.strip().lower():\n",
        "            correct_count += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_count / len(test_df)\n",
        "    print(f\"Accuracy: {accuracy:.2f} ({correct_count} out of {len(test_df)})\")\n"
      ],
      "metadata": {
        "id": "olY3AMdE6UsO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to properly format and load the JSON messages\n",
        "def load_json(messages):\n",
        "    # Replace single quotes with double quotes\n",
        "    messages = messages.replace(\"'\", '\"')\n",
        "    # Escape any double quotes inside strings\n",
        "    messages = messages.replace('\\\"', '\\\\\"')\n",
        "    return json.loads(messages)\n",
        "\n",
        "# Function to get the winner from the assistant's messages\n",
        "def get_assistant_winner(messages):\n",
        "    messages = load_json(messages)\n",
        "    for message in messages:\n",
        "        print(message)\n",
        "        if messages['role'] == 'assistant':\n",
        "            # Extract the winner's name from the assistant's message\n",
        "            return message['content'].split(' ')[0]\n",
        "\n",
        "# Function to check if the model's response is correct\n",
        "def is_correct(model_response, assistant_winner):\n",
        "    return assistant_winner in model_response\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv('gpt_with_explanations_responses.csv')\n",
        "\n",
        "# Count the correct predictions\n",
        "correct_predictions = 0\n",
        "for index, row in df.iterrows():\n",
        "    assistant_winner = get_assistant_winner(row['messages'])\n",
        "    model_is_correct = is_correct(row['ModelResponse'], assistant_winner)\n",
        "    correct_predictions += int(model_is_correct)\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = correct_predictions / len(df)\n",
        "print(f'Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "-jY0GtbltKvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV files\n",
        "test_df = pd.read_csv('test_data_chat_format.csv')\n",
        "model_df = pd.read_csv('gpt_model_with_responses.csv')\n",
        "\n",
        "# Ensure both DataFrames have the same number of rows\n",
        "if len(test_df) != len(model_df):\n",
        "    print(\"The DataFrames have different lengths. Make sure both CSV files have the same number of rows.\")\n",
        "else:\n",
        "    correct_count = 0\n",
        "\n",
        "    for i in range(len(test_df)):\n",
        "        # Extract assistant's response and get the winner's name\n",
        "        assistant_response = eval(test_df.loc[i, 'messages'])[-1]['content']\n",
        "        assistant_winner = assistant_response.split()[0]  # Get the first word, assuming it's the winner's name\n",
        "\n",
        "        # Extract model's response and get the winner's name\n",
        "        model_response = model_df.loc[i, 'ModelResponse']\n",
        "        model_winner = model_response.split()[0]  # Get the first word, assuming it's the winner's name\n",
        "\n",
        "        # Compare winners\n",
        "        if assistant_winner.strip().lower() == model_winner.strip().lower():\n",
        "            correct_count += 1\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = correct_count / len(test_df)\n",
        "    print(f\"Accuracy: {accuracy:.2f} ({correct_count} out of {len(test_df)})\")\n"
      ],
      "metadata": {
        "id": "uxx9pnUf9MMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(df):\n",
        "    correct = 0\n",
        "    for index, row in df.iterrows():\n",
        "        # Extract the loser's name from the prompt\n",
        "        # Assumes the loser is the second player mentioned in the prompt\n",
        "        loser_match = re.search(r'is playing Player ([\\w\\s.]+) \\(', row['prompt'])\n",
        "        if not loser_match:\n",
        "            continue  # Skip this row if the loser's name cannot be extracted\n",
        "        loser_name = loser_match.group(1).strip()\n",
        "\n",
        "        # Count occurrences of winner's and loser's name in the ModelResponse\n",
        "        winner_count = row['ModelResponse'].count(row['completion'])\n",
        "        loser_count = row['ModelResponse'].count(loser_name)\n",
        "\n",
        "        # Classify as correct if the winner's name appears more times than the loser's\n",
        "        if winner_count < loser_count:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(df)\n",
        "\n",
        "accuracy = calculate_accuracy(test_df)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "_9DuMTahpVlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regex pattern to match betting odds following a player's name\n",
        "betting_pattern = r'(\\b\\w+\\s\\w+\\b)\\s*(\\d+\\.\\d+)'\n",
        "\n",
        "# Function to calculate accuracy based on regex matches\n",
        "def calculate_accuracy(df):\n",
        "    correct = 0\n",
        "    for index, row in df.iterrows():\n",
        "        # Find all matches of player name and betting odds\n",
        "        matches = re.findall(betting_pattern, row['ModelResponse'])\n",
        "        # Create a dictionary of player name to their associated odds\n",
        "        player_odds = {match[0]: float(match[1]) for match in matches}\n",
        "\n",
        "        # Determine if the winner is mentioned with lower odds than the loser\n",
        "        winner = row['completion']\n",
        "        loser_match = re.search(r'is playing Player ([\\w\\s.]+) \\(', row['prompt'])\n",
        "        if not loser_match:\n",
        "            continue  # Skip this row if the loser's name cannot be extracted\n",
        "        loser = loser_match.group(1).strip()\n",
        "        print(player_odds)\n",
        "\n",
        "        # Check if both winner and loser are in the player_odds and if winner's odds are lower\n",
        "        if winner in player_odds and loser in player_odds and player_odds[winner] < player_odds[loser]:\n",
        "            correct += 1\n",
        "\n",
        "    return correct / len(df)\n",
        "\n",
        "accuracy = calculate_accuracy(test_df)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "D7F7RAXttY2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate accuracy\n",
        "def calculate_accuracy(df):\n",
        "    correct = 0\n",
        "    incorrect = 0\n",
        "    for index, row in df.iterrows():\n",
        "        loser_match = re.search(r'is playing Player ([\\w\\s.]+) \\(', row['prompt'])\n",
        "        if not loser_match:\n",
        "            continue  # Skip this row if the loser's name cannot be extracted\n",
        "        loser_name = loser_match.group(1).strip()\n",
        "        if row['ModelResponse'] == row['completion']:\n",
        "            correct += 1\n",
        "        elif row['ModelResponse'] == loser_name:\n",
        "            incorrect += 1\n",
        "    print(correct)\n",
        "    return correct / (correct + incorrect)\n",
        "\n",
        "accuracy = calculate_accuracy(test_df)\n",
        "print(f\"Model accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "oH_3WlhqkMDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other helpful functions for data/file conversions"
      ],
      "metadata": {
        "id": "Bvfq6NXgXFdc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert list to csv\n",
        "\n",
        "def list_to_csv(strings, filename):\n",
        "    # Open the file in write mode\n",
        "    with open(filename, 'w', newline='') as file:\n",
        "        # Create a CSV writer\n",
        "        writer = csv.writer(file)\n",
        "        # Write each string in a separate row\n",
        "        for string in strings:\n",
        "            writer.writerow([string])\n",
        "\n",
        "# Convert the list to a CSV file\n",
        "list_to_csv(responses, 'output.csv')\n"
      ],
      "metadata": {
        "id": "kWJ5E_am1feu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert df to csv\n",
        "\n",
        "test_df.to_csv('gpt_responses_with_explanations2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "zKICsmxDoX3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert jsonl to csv\n",
        "\n",
        "with open('filename.jsonl', 'r') as json_file:\n",
        "    json_list = [json.loads(json_str) for json_str in json_file]\n",
        "\n",
        "headers = json_list[0].keys()\n",
        "\n",
        "# Write the data to a CSV file\n",
        "with open('filename.csv', 'w', newline='') as csv_file:\n",
        "    writer = csv.DictWriter(csv_file, fieldnames=headers)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for json_item in json_list:\n",
        "        writer.writerow(json_item)\n"
      ],
      "metadata": {
        "id": "YmTImp7ZYX7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the 'content' of 'assistant' from the string\n",
        "# which is in a JSON format\n",
        "\n",
        "df = pd.read_csv('train_explanation.csv')\n",
        "\n",
        "def extract_assistant_content(s):\n",
        "    try:\n",
        "        # Parse the string as JSON\n",
        "        conversations = json.loads(s.replace(\"'\", \"\\\"\"))\n",
        "        # Filter out the content where 'role' is 'assistant'\n",
        "        assistant_responses = [conv['content'] for conv in conversations if conv['role'] == 'assistant']\n",
        "        return assistant_responses\n",
        "    except json.JSONDecodeError:\n",
        "        return []  # Return an empty list if there's a JSON decoding error\n",
        "\n",
        "# Apply the function to each row in the DataFrame and concatenate the results\n",
        "assistant_contents = []\n",
        "for row in df.itertuples(index=False):\n",
        "    assistant_contents.extend(extract_assistant_content(row[0]))\n",
        "\n",
        "assistant_contents\n"
      ],
      "metadata": {
        "id": "PyGLoOeWYtiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To finetune the gpt model with explanations, we had GPT 4 generate explanations for all of our training data match outcomes based on its existing knowledge about the player (ex.  'Rublev A. won the match against Berankis R. due to his powerful forehand and high-ranking experience.') Once we had the data in an array (explanations), we could insert it into the training dataset for finetuning.\n"
      ],
      "metadata": {
        "id": "hK2CHWXXZCUu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update csv with explanations\n",
        "\n",
        "csv_file_path = 'train_explanation.csv'\n",
        "explanations = [] # insert with explanations (from GPT 4 for training)\n",
        "\n",
        "\n",
        "def update_csv_with_explanations(csv_path, explanations_list):\n",
        "    # Read the CSV file\n",
        "    with open(csv_path, mode='r', encoding='utf-8') as file:\n",
        "        csv_reader = csv.DictReader(file)\n",
        "        rows = list(csv_reader)\n",
        "\n",
        "    # Update the content with explanations\n",
        "    for i, row in enumerate(rows):\n",
        "        if i < len(explanations):\n",
        "            row['content'] = explanations_list[i]\n",
        "\n",
        "    # Write the updated content back to the CSV\n",
        "    with open(csv_path, mode='w', encoding='utf-8', newline='') as file:\n",
        "        fieldnames = rows[0].keys()\n",
        "        csv_writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "        csv_writer.writeheader()\n",
        "        for row in rows:\n",
        "            csv_writer.writerow(row)\n",
        "\n",
        "# Call the function with the path to your CSV file and the explanations array\n",
        "update_csv_with_explanations(csv_file_path, explanations)\n"
      ],
      "metadata": {
        "id": "8WsZBfhAY2o9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}